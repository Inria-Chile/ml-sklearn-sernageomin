{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align='left' style=\"width:38%;overflow:hidden;\">\n",
    "<a href='http://inria.fr'>\n",
    "<img src='https://github.com/lmarti/jupyter_custom/raw/master/imgs/inr_logo_rouge.png' alt='Inria logo' title='Inria'/>\n",
    "</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning with `scikit-learn`\n",
    "\n",
    "# Lab 01. Introduction to Machine learning\n",
    "\n",
    "In this lab we will firstly see a simple example of a classification problem.\n",
    "\n",
    "Afterwards, we will focus on three different questions related to ML:\n",
    "\n",
    "\n",
    "1. Learn how to use a sklearn to solve ML tasks (classification and regression)\n",
    "2. Illustrate important concepts in ML (imputation, validation, standarization, etc.) \n",
    "3. Learn how to create sophisticated ML pipelines for real-world problems\n",
    "\n",
    "\n",
    "We will use material from the books:\n",
    "\n",
    "- \"Hands-On Machine Learning with Scikit-Learn and TensorFlow. Concepts, Tools, and Techniques to Build Intelligent Systems\" by Aurélien Géron. http://shop.oreilly.com/product/0636920052289.do \n",
    "\n",
    "- \"Deep Learning with Python\" by F. Chollet. https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438\n",
    "\n",
    "which are recommended as Bibliography of the course.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pylab as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Enables interaction with the plots\n",
    "%matplotlib notebook\n",
    "\n",
    "# These are modules contained in sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import ClassifierMixin\n",
    "\n",
    "# Imputation methods\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Methods for preprocessing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import binarize\n",
    "\n",
    "# Feature selection Methods\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Methods for classifier validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn import metrics\n",
    "\n",
    "# Datasets in sklearn\n",
    "import sklearn.datasets as data_load\n",
    "\n",
    "# Pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "# Enables interactivity with the plots\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the python libraries required to solve the problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create two sets of points (XA,yA)  and (XB,yB) corresponding to two different classes\n",
    "\n",
    "number_points_Class_A = 50\n",
    "number_points_Class_B = 50\n",
    "\n",
    "# Font size\n",
    "fsize = 20\n",
    "\n",
    "# Points in Class A\n",
    "xA = 20 * np.random.rand(number_points_Class_A)\n",
    "shiftA = 20 * np.random.rand(number_points_Class_A)\n",
    "yA = (4 + xA) / 2.0 - shiftA - 0.1\n",
    "\n",
    "# Points in Class B\n",
    "xB = 20 * np.random.rand(number_points_Class_B)\n",
    "shiftB = 20 * np.random.rand(number_points_Class_B)\n",
    "yB = (4 + xB) / 2.0 + shiftB + 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The points in the two classes are visualized with different colors\n",
    "# Points in Class I in blue. Points in Class II in red\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(xA, yA, 'ro')\n",
    "plt.plot(xB, yB, 'bs')\n",
    "\n",
    "blue_patch = mpatches.Patch(color='blue', label='Class I')\n",
    "red_patch = mpatches.Patch(color='red', label='Class II')\n",
    "plt.legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "plt.xlabel(r'$x$', fontsize=fsize)\n",
    "plt.ylabel(r'$y$', fontsize=fsize)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(xA, yA, 'bo')\n",
    "plt.plot(xB, yB, 'bo')\n",
    "\n",
    "plt.xlabel(r'$x$', fontsize=fsize)\n",
    "plt.ylabel(r'$y$', fontsize=fsize)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.linspace(0, 20, 2000)\n",
    "y1 = (4 + x1) / 2.0\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(xA, yA, 'bo')\n",
    "plt.plot(xB, yB, 'bo')\n",
    "\n",
    "plt.xlabel(r'$x$', fontsize=fsize)\n",
    "plt.ylabel(r'$y$', fontsize=fsize)\n",
    "\n",
    "plt.plot(x1, y1, 'g', lw=4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show that there exists a hyperplane that allows to perfectly divide points in the two classes.\n",
    "\n",
    "In 2-d this hyperplane corresponds to a line that is represented in green."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.linspace(0, 20, 2000)\n",
    "y1 = (4 + x1) / 2.0\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(xA, yA, 'ro')\n",
    "plt.plot(xB, yB, 'bs')\n",
    "\n",
    "blue_patch = mpatches.Patch(color='blue', label='Class I')\n",
    "red_patch = mpatches.Patch(color='red', label='Class II')\n",
    "plt.legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "plt.xlabel(r'$x$', fontsize=fsize)\n",
    "plt.ylabel(r'$y$', fontsize=fsize)\n",
    "\n",
    "# Show hyperplane dividing the classes\n",
    "plt.plot(x1, y1, 'g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we create a more difficult classification problem\n",
    "# where the two region classes overlap.\n",
    "\n",
    "# Font size\n",
    "fsize = 20\n",
    "\n",
    "# Points in Class A\n",
    "xA1 = 20 * np.random.rand(number_points_Class_A)\n",
    "shiftA1 = 20 * np.random.rand(number_points_Class_A)\n",
    "yA1 = (4 + xA1) / 2.0 - shiftA1 + 5.0\n",
    "\n",
    "# Points in Class B\n",
    "xB1 = 20 * np.random.rand(number_points_Class_B)\n",
    "shiftB1 = 20 * np.random.rand(number_points_Class_B)\n",
    "yB1 = (4 + xB1) / 2.0 + shiftB1 - 5.0\n",
    "\n",
    "# Hyperplane dividing the two classes\n",
    "x1 = np.linspace(0, 20, 2000)\n",
    "y1 = (4 + x1) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The points corresponding to the two classes are plotted\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(xA1, yA1, 'ro')\n",
    "plt.plot(xB1, yB1, 'bs')\n",
    "\n",
    "blue_patch = mpatches.Patch(color='blue', label='Class I')\n",
    "red_patch = mpatches.Patch(color='red', label='Class II')\n",
    "plt.legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "plt.xlabel(r'$x$', fontsize=fsize)\n",
    "plt.ylabel(r'$y$', fontsize=fsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the same hyperplane to divide points from the two classes\n",
    "# However, as it can be appreciated, the classification provided by this hyperplane\n",
    "# is not perfect\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(xA1, yA1, 'ro')\n",
    "plt.plot(xB1, yB1, 'bs')\n",
    "\n",
    "blue_patch = mpatches.Patch(color='blue', label='Class I')\n",
    "red_patch = mpatches.Patch(color='red', label='Class II')\n",
    "plt.legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "plt.xlabel(r'$x$', fontsize=fsize)\n",
    "plt.ylabel(r'$y$', fontsize=fsize)\n",
    "\n",
    "# Show hyperplane dividing the classes\n",
    "plt.plot(x1, y1, 'g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we create a more difficult classification problem\n",
    "# where the two region classes seem to overlap and where\n",
    "# it is not evident that a linear separator exists.\n",
    "\n",
    "# Font size\n",
    "fsize = 20\n",
    "\n",
    "# Points in Class A\n",
    "xA2 = 20 * np.random.rand(number_points_Class_A)\n",
    "shiftA2 = 20 * np.random.rand(number_points_Class_A)\n",
    "yA2 = 20 * np.cos(0.2 * np.pi * xA2) - shiftA2\n",
    "\n",
    "# Points in Class B\n",
    "xB2 = 20 * np.random.rand(number_points_Class_B)\n",
    "shiftB2 = 20 * np.random.rand(number_points_Class_B)\n",
    "yB2 = 20 * np.cos(0.2 * np.pi * xB2) + shiftB2\n",
    "\n",
    "# Sinusoidal curve dividing the two classes\n",
    "x2 = np.linspace(0, 20, 2000)\n",
    "y2 = 20 * np.cos(0.2 * np.pi * x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The points corresponding to the two classes are plotted\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(xA2, yA2, 'ro')\n",
    "plt.plot(xB2, yB2, 'bs')\n",
    "\n",
    "blue_patch = mpatches.Patch(color='blue', label='Class I')\n",
    "red_patch = mpatches.Patch(color='red', label='Class II')\n",
    "plt.legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "plt.xlabel(r'$x$', fontsize=fsize)\n",
    "plt.ylabel(r'$y$', fontsize=fsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we plot in green the same line that was previously used to classify the previous data\n",
    "# It can be seen that it does not provide a good separation of the data\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(xA2, yA2, 'ro')\n",
    "plt.plot(xB2, yB2, 'bs')\n",
    "\n",
    "blue_patch = mpatches.Patch(color='blue', label='Class I')\n",
    "red_patch = mpatches.Patch(color='red', label='Class II')\n",
    "plt.legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "plt.xlabel(r'$x$', fontsize=fsize)\n",
    "plt.ylabel(r'$y$', fontsize=fsize)\n",
    "\n",
    "# Show the hyperplane previously computed  dividing the classes\n",
    "plt.plot(x1, y1, 'g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we also show a curve that makes a perfect separation of the data\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(xA2, yA2, 'ro')\n",
    "plt.plot(xB2, yB2, 'bs')\n",
    "\n",
    "blue_patch = mpatches.Patch(color='blue', label='Class I')\n",
    "red_patch = mpatches.Patch(color='red', label='Class II')\n",
    "plt.legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "plt.xlabel(r'$x$', fontsize=fsize)\n",
    "plt.ylabel(r'$y$', fontsize=fsize)\n",
    "\n",
    "# Show the curve dividing the classes\n",
    "plt.plot(x2, y2, 'm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A very simple binary classification problem \n",
    "For our first classification problem, we will take the initial, easy problem back. As previously, we can divide create a perfect classifier by using a hyperplane (a line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In 2-d this hyperplane corresponds to a line that is represented in green.\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(xA, yA, 'rs', label=\"Class II\")\n",
    "plt.plot(xB, yB, 'bs', label=\"Class I\")\n",
    "\n",
    "plt.xlabel(r'$x$', fontsize=fsize)\n",
    "plt.ylabel(r'$y$', fontsize=fsize)\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# Show hyperplane dividing the classes\n",
    "plt.plot(x1, y1, 'g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning simple classifiers using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The common syntax for learning a classifier in sklearn implies three steps: \n",
    "\n",
    "- Declaration of the classifier\n",
    "- Fitting the classifier from the data and the labels\n",
    "- Using the classifier to predict new data\n",
    "\n",
    "In the example below we learn a logistic regression classifier that separates the two classes presented above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will create the training data that is needed since we are working with a supervised classification algorithm. By uncommenting the \"print\" line you can check the size of the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the labels for our two classes A and B. Variable c will keep the\n",
    "# labels of all points.\n",
    "# The points in Class A will have label 1 assigned and the points in Class B,\n",
    "# will have label 0.\n",
    "\n",
    "c = np.hstack((np.ones((number_points_Class_A)),\n",
    "               np.zeros((number_points_Class_B))))\n",
    "#print(c.shape)\n",
    "\n",
    "# We create the training data concatenating examples from the two classes XA and XB\n",
    "training_data = np.hstack((np.vstack((xA, yA)), np.vstack((xB,\n",
    "                                                           yB)))).transpose()\n",
    "#print(training_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We declare the classifier (it is a logistic regression classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learn the classifier. In sklearn this is done using the function fit(data,classes)\n",
    "It will update the \"lr\" model using the data from training_data and the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(training_data,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the logistic regression classifier lr to predict the classes of the dataset. In the example below we predict the classes of the same dataset used for training (training_data) but this is not a realistic case. Usually we predict the classes of data that was not used for training (called the test data). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions of the classifier on the training data\n",
    "prediction_training = lr.predict(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Visualization of the predictions given by the classifier\n",
    "\n",
    "A classifier is accurate if for most of the points the class predicted coincide with the real classes. In this sense, for a binary problem, each prediction can be of four types:\n",
    "\n",
    "1) true positive: A positive point is given positive prediction\n",
    "\n",
    "2) false positive:  A negative point is given positive prediction\n",
    "\n",
    "3) true negative: A negative point is given negative prediction\n",
    "\n",
    "4) false negative:  A positive point is given  negative prediction\n",
    "\n",
    "Below, we put each classifier prediction in one of the four groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Font size\n",
    "fsize = 20\n",
    "\n",
    "# We identify the points that were correctly and wrongly classified\n",
    "true_positive = np.where(prediction_training[:number_points_Class_A] == 1)[0]\n",
    "false_positive = np.where(prediction_training[:number_points_Class_A] == 0)[0]\n",
    "true_negative = number_points_Class_A + np.where(\n",
    "    prediction_training[number_points_Class_A:] == 0)[0]\n",
    "false_negative = number_points_Class_A + np.where(\n",
    "    prediction_training[number_points_Class_A:] == 1)[0]\n",
    "\n",
    "# Points are shown in different colors according to classification\n",
    "plt.figure()\n",
    "plt.plot(training_data[true_positive, 0], training_data[true_positive, 1],\n",
    "         'ro')\n",
    "plt.plot(training_data[false_positive, 0], training_data[false_positive, 1],\n",
    "         'go')\n",
    "plt.plot(training_data[true_negative, 0], training_data[true_negative, 1],\n",
    "         'bs')\n",
    "plt.plot(training_data[false_negative, 0], training_data[false_negative, 1],\n",
    "         'ms')\n",
    "\n",
    "blue_patch = mpatches.Patch(color='blue', label='True pos.')\n",
    "green_patch = mpatches.Patch(color='green', label='False pos.')\n",
    "red_patch = mpatches.Patch(color='red', label='True neg.')\n",
    "magenta_patch = mpatches.Patch(color='magenta', label='False neg.')\n",
    "\n",
    "plt.legend(\n",
    "    handles=[blue_patch, green_patch, red_patch, magenta_patch],\n",
    "    #bbox_to_anchor=(0, 1),\n",
    "    ncol=4,\n",
    "    loc='upper center')\n",
    "\n",
    "plt.xlabel(r'$x$', fontsize=fsize)\n",
    "plt.ylabel(r'$y$', fontsize=fsize)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Below we want to learn a k-nearest-neighbor classifier from the train_data and use it \n",
    "to predict the classes of the test data. The problem has two classes. The positive class\n",
    "is formed by vectors with 4 or more zeros. The negative class is formed by vectors of 3 or less zeros.\n",
    "\n",
    "Below, replace the XXX in LINE 1, LINE 2, and LINE 3 to get the predictions for the test data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Our training dataset has 10 instances\n",
    "# Each instance has 5 variables\n",
    "train_data = np.array([[1, 1, 1, 1, 1], \n",
    "                       [1, 1, 1, 0, 1], \n",
    "                       [1, 1, 0, 1, 1],\n",
    "                       [1, 1, 1, 0, 1], \n",
    "                       [1, 0, 1, 1, 1], \n",
    "                       [0, 0, 0, 0, 0],\n",
    "                       [0, 0, 0, 1, 0], \n",
    "                       [0, 0, 0, 0, 1], \n",
    "                       [1, 0, 0, 0, 0],\n",
    "                       [0, 1, 0, 0, 0]])\n",
    "\n",
    "# We have the classes for the ten instances\n",
    "# The first five instances belong to class 1\n",
    "# the rest to class 2\n",
    "\n",
    "train_classes = np.array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])\n",
    "test_data = np.random.randint(low=0, high=2, size=(10, 5))\n",
    "\n",
    "print(\"test_data\", XXX) \n",
    "\n",
    "knn = KNeighborsClassifier()  # LINE 1\n",
    "knn.fit(XXX, XXX)  # LINE 2\n",
    "\n",
    "prediction_test = knn.predict(XXX)  # LINE 3\n",
    "\n",
    "print(\"predictions for test data\", prediction_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A real-word classification problem and more sophisticated validation schemes\n",
    "\n",
    "\n",
    "sklearn also contains a number of databases that can be used to test the algorithms. We will use some of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can check which are the datasets included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Available datasets:\")\n",
    "[name for name in data_load.__all__ if \"load\" in name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Inspecting the Real-World datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the breast cancer dataset, included in UCI ML Repository https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)\n",
    "\n",
    "\n",
    "It has been used for the application of ML to Cancer diagnosis and prognosis: http://pages.cs.wisc.edu/~olvi/uwmp/cancer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset is loaded\n",
    "breast_cancer_data = data_load.load_breast_cancer()\n",
    "\n",
    "# Display options\n",
    "#  These options determine the way floating point numbers,\n",
    "#  arrays and other NumPy objects are displayed\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a good practice to inspect the dataset before applying any ML technique, its header and also the characteristics of the data. \n",
    "\n",
    "For example, it is very important to know the number of attributes (variables), their type, and also the size of the data (number of instances). In the description that is shown below you will find this information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some information about the dataset, understand what we are aiming for\n",
    "print(breast_cancer_data[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We analyze more details of the database. Rows define observations (instances of our classification problem). Columns represent variables captured in each observation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_cancer_data[\"data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the rows shown above that the range of values change among the columns. Some columns seem to have values between 0 and 1 and others much higher values. This has to be taken into account for the application of the classifiers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this a binary problem, classes are either 0: the tumor is malign or 1: the tumor is benign "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classes in the database\n",
    "breast_cancer_data[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous analysis, notice that we have used:   data[\"data\"] to visualize the features and data[\"target\"] to see the classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing missing data\n",
    "\n",
    "Imputation methods serve to substitute missing values in the data. In the example below we define a small dataset of three variables, six instances and three missing values (NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = np.array([[ np.nan,  7,      6],\n",
    "                    [ 5,       89,     13],\n",
    "                    [ 23,      12,     213],\n",
    "                    [ 2,       87,     np.nan],\n",
    "                    [ 8,       101,    71],\n",
    "                    [ 13,      np.nan, 20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Imputer\" class allows to impute the missing data. It implements three strategies: \"mean\", \"median\", and \"most_frequent\".\n",
    "If works like the classifiers:\n",
    "\n",
    "1) Define the imputer\n",
    "\n",
    "2) Fit the imputer to the data using the function \"fit\"\n",
    "\n",
    "3) Impute the data using the function \"transform\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the imputer\n",
    "mean_imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n",
    "\n",
    "#  Fit the imputer\n",
    "mean_imputer.fit(my_data)\n",
    "\n",
    "# Transform (impute) the data\n",
    "imputed_data = mean_imputer.transform(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the data can improve the accuracy of some classifiers. This can be done in a similar way in which we define the classifiers:\n",
    "\n",
    "1) Define the scaler\n",
    "\n",
    "2) Fit the scaler to the data using the function \"fit\"\n",
    "\n",
    "3) Scale the data using the function \"transform\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the scaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# Fit the data to scaler\n",
    "scaler.fit(X=breast_cancer_data[\"data\"])\n",
    "\n",
    "# Scale the data\n",
    "scaled_data = scaler.transform(X=breast_cancer_data[\"data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample process can be done in only two steps using the functions fit_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the scaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# Fit and scale the data\n",
    "scaled_data = scaler.fit_transform(X=breast_cancer_data[\"data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful pre-processing algorithm is to binarize the data. It is NOT defined as classifiers, imputer, and scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarized_data = binarize(scaled_data)\n",
    "print(binarized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection \n",
    "\n",
    "Feature selection is an important step in traditional classifiers. I allows to reduce the dimensionality of the data. This can be done in a similar way in which we define the classifiers:\n",
    "\n",
    "1) Define the feature selection method\n",
    "\n",
    "2) Fit the feature-selection method to the data using the function \"fit\"\n",
    "\n",
    "3) Select  the data using the function \"transform\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below a an information metric approach is used to select the two most informative features for the classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The feature selection method is defined\n",
    "feature_selection = SelectKBest(f_classif, k=2)\n",
    "\n",
    "# Using fit_transform we select fit the selector to the data and finally select features\n",
    "new_features = feature_selection.fit_transform(scaled_data,\n",
    "                                               breast_cancer_data[\"target\"])\n",
    "print(new_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Now that we have understood what a classification problem is, and have seen a couple of simple examples, let's put into practice what we have learned. For this, we will firstly study a chosen problem before creating a model that performs a classification task. More specifically, we are carrying out the following tasks:\n",
    "\n",
    "1) Select a DB from the set available in sklearn\n",
    "\n",
    "2) Briefly, describe the chosen DB; number of features, classes, observations, ...\n",
    "\n",
    "3) Create and evaluate a classifier\n",
    "\n",
    "    3.1) Choose a classification algorithm from those available in sklearn\n",
    "    3.2) Train and test the quality of the model\n",
    "    \n",
    "4) Try to improve the accuracy of the model created\n",
    "\n",
    "    4.1) Try to tweak the parameters in the classifier\n",
    "    4.2) Try to improve the data quality by using a pre-processing method\n",
    "    4.3) Change the classification algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, lets see what the available classifiers are:\n",
    "\n",
    "AdaBoostClassifier: <class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'>\n",
    "\n",
    "BaggingClassifier: <class 'sklearn.ensemble.bagging.BaggingClassifier'>\n",
    "\n",
    "BernoulliNB: <class 'sklearn.naive_bayes.BernoulliNB'>\n",
    "\n",
    "CalibratedClassifierCV: <class 'sklearn.calibration.CalibratedClassifierCV'>\n",
    "\n",
    "DecisionTreeClassifier: <class 'sklearn.tree.tree.DecisionTreeClassifier'>\n",
    "\n",
    "ExtraTreeClassifier: <class 'sklearn.tree.tree.ExtraTreeClassifier'>\n",
    "\n",
    "ExtraTreesClassifier: <class 'sklearn.ensemble.forest.ExtraTreesClassifier'>\n",
    "\n",
    "GaussianNB: <class 'sklearn.naive_bayes.GaussianNB'>\n",
    "\n",
    "GaussianProcessClassifier: <class 'sklearn.gaussian_process.gpc.GaussianProcessClassifier'>\n",
    "\n",
    "GradientBoostingClassifier: <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>\n",
    "\n",
    "KNeighborsClassifier: <class 'sklearn.neighbors.classification.KNeighborsClassifier'>\n",
    "\n",
    "LabelPropagation: <class 'sklearn.semi_supervised.label_propagation.LabelPropagation'>\n",
    "\n",
    "LabelSpreading: <class 'sklearn.semi_supervised.label_propagation.LabelSpreading'>\n",
    "\n",
    "LinearDiscriminantAnalysis: <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>\n",
    "\n",
    "LinearSVC: <class 'sklearn.svm.classes.LinearSVC'>\n",
    "\n",
    "LogisticRegression: <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
    "\n",
    "LogisticRegressionCV: <class 'sklearn.linear_model.logistic.LogisticRegressionCV'>\n",
    "\n",
    "MLPClassifier: <class 'sklearn.neural_network.multilayer_perceptron.MLPClassifier'>\n",
    "\n",
    "MultinomialNB: <class 'sklearn.naive_bayes.MultinomialNB'>\n",
    "\n",
    "NearestCentroid: <class 'sklearn.neighbors.nearest_centroid.NearestCentroid'>\n",
    "\n",
    "NuSVC: <class 'sklearn.svm.classes.NuSVC'>\n",
    "\n",
    "PassiveAggressiveClassifier: <class 'sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier'>\n",
    "\n",
    "Perceptron: <class 'sklearn.linear_model.perceptron.Perceptron'>\n",
    "\n",
    "QuadraticDiscriminantAnalysis: <class 'sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis'>\n",
    "\n",
    "RadiusNeighborsClassifier: <class 'sklearn.neighbors.classification.RadiusNeighborsClassifier'>\n",
    "\n",
    "RandomForestClassifier: <class 'sklearn.ensemble.forest.RandomForestClassifier'>\n",
    "\n",
    "RidgeClassifier: <class 'sklearn.linear_model.ridge.RidgeClassifier'>\n",
    "\n",
    "RidgeClassifierCV: <class 'sklearn.linear_model.ridge.RidgeClassifierCV'>\n",
    "\n",
    "SGDClassifier: <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'>\n",
    "\n",
    "SVC: <class 'sklearn.svm.classes.SVC'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remember to import the classifier from the sklearn library. (See the first cell in the notebook for importing examples)\n",
    "\n",
    "from x import y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the chosen database as we did with the breast cancer dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Investigate the characteristics of the database\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model with the loaded data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the accuracy_score function in sklearn to obtain a numeric value of how good the model is\n",
    "\n",
    "accuracy_score(<real_classes>, <predictions>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have a reference value of the quality of the model, try to tune the parameters of the algorithm so that the model quality improves. Also, add a preprocessor, or change the classification algorithm."
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "nteract": {
   "version": "0.21.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
